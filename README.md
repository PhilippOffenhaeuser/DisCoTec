What is this project about?
---------------------------
[![Build Status](https://simsgs.informatik.uni-stuttgart.de/jenkins/buildStatus/icon?job=DisCoTec-master)](https://simsgs.informatik.uni-stuttgart.de/jenkins/view/DisCoTec/job/DisCoTec-master/)

This project contains __DisCoTec__, a code for running the distributed sparse grid combination technique with MPI parallelization. While it originates from the excellent [SGpp project](https://github.com/SGpp/SGpp), all the parallelization makes it a very different code, such that it is its own project now.

Guidelines
---------
* Please develop new features in a new branch and then create a merge request to
  notify other users about the changes. So everyone can check whether there are
  side-effects to other branches.
* Of course, you are still very welcome to directly fix obvious bugs in the
  master branch.
* Before merging new features to the master branch, please make sure that they
are sufficiently commented and extensively tested.

Requirements
--------------
scons (Python3, >= 3.)
libboost-all-dev (>= 1.60)
libmpich-dev (>= 3.2-6), or other MPI library


Installation instructions: 
--------------------------
Compile by updating and running
```
./compile.sh
``` 
(load dependencies as required).

For debugging, you may change to `OPT=0` for the `scons` command, which will turn on assertions and debug symbols. If you want only debug symbols, go with `CPPFLAGS=-g` in the `scons` call. Using `DEBUG_OUTPUT=1` results in absurd amounts of debug output, which you may only want in case you are developing a new feature.

To run the compiled tests, go to folder `distributedcombigrid/tests` and run
```
mpiexec -np 9 ./test_distributedcombigrid_boost
```
where you can use all the parameters of the boost test suite.

The scons command also generates `Makefile`s for all the examples. If you want to build one of the examples, make sure to edit its `Makefile.template` so that the compiler calls match the ones generated by `scons`. Run `make` in the example folder.

To clean the compiled files, use
```
scons -c && rm -r .scon*
```


GENE submodules as dependencies for GENE examples
----------------
There are gene versions as submodules: a linear one in the gene_mgr folder, and 
a nonlinear one in gene-non-linear. To get them, you need access to their 
respective repos. Then go into the folder and

```
git submodule init
git submodule update
```
and then you just hope you can `make` it by adapting the local library flags ;)

Third-Level-Manager
-------------------
The third-level-manager handles the synchronization and data transfer between
the systems. The following steps will guide you from compilation to execution:

1. To compile first navigate to combi/distributedcombigrid/third_level_manager.
   The folder contains a Makefile.template which you can adjust to make it
   compatible with your maschine.

2. The manager takes a .ini file as an input parameter. The same folder as in 1.
   holds the example file `example.ini` where the number of systems and the port
   on which the manager listens can be adjusted. Currently, only 2 systems are
   supported.

3. Use the run script to execute the manager. You can pass your own parameter
   file to the script whereas by default it reads the example.ini.

On Hazel Hen
------------

### Prerequisites

* load modules: PrgEnv-gnu, scons, python 2.7

* Check loaded modules with:
  `module list`

* E.g. to change the default cray compiler to gnu:
  `module switch PrgEnv-cray PrgEnv-gnu`

* Allow dynamic linking:
  `export CRAYPE_LINK_TYPE=dynamic`

### Compilation

Unfortunately, the boost module on Hazel Hen does not work with our code any
more. Hence, you have to install boost yourself, e.g. use version >= 1.58
(always compile boost with the intel compiler `PrgEnv-intel`, otherwise there
will be linkage errors when compiling the examples) Do not forget to set boost
paths in SConfigure, i.e. BOOST_INCLUDE_PATH, BOOST_LIBRARY_PATH.

Compile by first adapting the compile.sh script to use the HLRS specific
command.

(the linking of the boost tests might fail. However this is not a problem, the
sg++ libraries should be there)

### Execution

Let's assume we want to run the example under
combi/distributedcombigrid/examples/distributed_third_level/ and distribute our
combischeme to **HLRS** and **helium**, while the third-level-manager is running
on helium at port 9999. The following steps are necessary:

1. Since data connections to the HLRS are not possible without using ssh
   tunnels, we set them up in advance. Run `ssh -R  9998:localhost:9999
   username@eslogin002.hww.de` from helium to log in to HLRS while creating an
   ssh tunnel.

2. Adjust the parameter files on HLRS and helium to fit the simulation. Use
   hostname=eslogin002 on HLRS and hostname=localhost on helium. Set
   dataport=9999 on both systems.

3. Run the third-level-manger on helium.

4. Connect to eslogin002 in a different terminal and run the forwarding
   script `forward.sh 9999 9998 pipe1`. This will forward the port 9998 to 9999
   on eslogin002. (We only need the local forwarding because the configuration
   of the ssh server on the HLRS does not allow us to access the ssh tunnel
   from a different host than eslogin002. Since our application runs on the
   compute nodes (for now) this detour is necessary.)

5. Start the simulation. The example directory holds a separate run file
   `run.sh` which needs to be modified to fit HLRS and helium. Also set the
   corresponding boost library location.
